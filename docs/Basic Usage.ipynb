{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Basic Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from autora.experimentalist.autora_experimentalist_example import Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include inline mathematics like this: $4 < 5$\n",
    "\n",
    "Include block mathematics like this (don't forget the empty lines above and below the block):\n",
    "\n",
    "$$  \n",
    "y + 1 = 4 \n",
    "$$\n",
    "\n",
    "... or this:\n",
    "\n",
    "\\begin{align}\n",
    "    p(v_i=1|\\mathbf{h}) & = \\sigma\\left(\\sum_j w_{ij}h_j + b_i\\right) \\\\\n",
    "    p(h_j=1|\\mathbf{v}) & = \\sigma\\left(\\sum_i w_{ij}v_i + c_j\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'.venv-1 (Python 3.12.3)' でセルを実行するには、 ipykernel パッケージが必要です。\n",
      "\u001b[1;31m'ipykernel' を Python 環境にインストールします。\n",
      "\u001b[1;31mコマンド: '/workspaces/autora-experimentalist-MAFT/.venv-1/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Random-Subset-Novelty (RSN)\n",
    "# Exploit = run greedy max-min novelty inside a small random window\n",
    "\n",
    "# Updated samplers\n",
    "# - Random-Subset Novelty (RSN)\n",
    "# - Stratified (under-covered bins) + RSN\n",
    "#\n",
    "# Usage: pick one at the bottom via:  sample = sample_rsn   or   sample = sample_stratified_rsn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Union, Optional, Sequence\n",
    "\n",
    "\n",
    "# ---------- helpers ----------\n",
    "\n",
    "def _anti_join(pool: pd.DataFrame, tested: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Remove rows already in `tested` from `pool` (schema must match).\"\"\"\n",
    "    if tested is None or tested.empty:\n",
    "        return pool.copy()\n",
    "    mask = ~pool.apply(tuple, axis=1).isin(tested.apply(tuple, axis=1))\n",
    "    return pool.loc[mask]\n",
    "\n",
    "\n",
    "def _grid_scalers(df: pd.DataFrame) -> tuple[pd.Series, pd.Series, pd.Index]:\n",
    "    \"\"\"Return (min, range, numeric_cols) computed on the (full) grid.\"\"\"\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    rc = df[num_cols].astype(float)\n",
    "    rc_min = rc.min()\n",
    "    rc_rng = (rc.max() - rc_min).replace(0, 1.0)\n",
    "    return rc_min, rc_rng, num_cols\n",
    "\n",
    "\n",
    "def _scale(df: pd.DataFrame, rc_min: pd.Series, rc_rng: pd.Series, cols: Sequence[str]) -> np.ndarray:\n",
    "    \"\"\"Scale selected numeric cols of df to [0,1] using grid stats.\"\"\"\n",
    "    return ((df[cols].astype(float) - rc_min) / rc_rng).to_numpy()\n",
    "\n",
    "\n",
    "def _greedy_maxmin_subset(\n",
    "    subset_df: pd.DataFrame,      # candidates subset (rows to choose from)\n",
    "    tested_arr: np.ndarray,       # scaled tested array (n_tested, d)\n",
    "    rc_min: pd.Series, rc_rng: pd.Series, cols: Sequence[str],\n",
    "    k: int,\n",
    "    rng: np.random.Generator\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Greedy farthest-first (k-center) selection on `subset_df` against `tested_arr`,\n",
    "    returning `k` rows from `subset_df`. Incremental O(kN).\n",
    "    \"\"\"\n",
    "    sub_arr = _scale(subset_df, rc_min, rc_rng, cols)\n",
    "\n",
    "    # current min distance to union (starts with tested only)\n",
    "    if tested_arr.size == 0:\n",
    "        best_d = np.full(len(sub_arr), np.inf)\n",
    "    else:\n",
    "        best_d = np.linalg.norm(sub_arr[:, None, :] - tested_arr[None, :, :], axis=2).min(axis=1)\n",
    "\n",
    "    # tiny jitter for deterministic tie-breaking\n",
    "    if np.any(np.isfinite(best_d)):\n",
    "        best_d = best_d + 1e-12 * rng.standard_normal(best_d.shape)\n",
    "\n",
    "    chosen: list[int] = []\n",
    "    k = min(k, len(sub_arr))\n",
    "    for _ in range(k):\n",
    "        i = int(np.argmax(best_d))\n",
    "        chosen.append(i)\n",
    "        # update distances with just the newly added point\n",
    "        d_new = np.linalg.norm(sub_arr - sub_arr[i], axis=1)\n",
    "        best_d = np.minimum(best_d, d_new)\n",
    "        best_d[chosen] = -np.inf\n",
    "\n",
    "    return subset_df.iloc[chosen].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _bin_keys(df: pd.DataFrame, cols: Sequence[str], bins: int = 10) -> pd.Series:\n",
    "    \"\"\"Create a coarse grid key per row by binning numeric cols.\"\"\"\n",
    "    if len(cols) == 0 or df.empty:\n",
    "        return pd.Series([\"_all\"] * len(df), index=df.index)\n",
    "    binned = [pd.cut(df[c], bins=bins, labels=False, include_lowest=True, duplicates=\"drop\") for c in cols]\n",
    "    key = pd.concat(binned, axis=1).astype(\"Int64\").astype(str).agg(\"-\".join, axis=1)\n",
    "    return key\n",
    "\n",
    "\n",
    "def _under_coverage_weights(candidates: pd.DataFrame, tested: pd.DataFrame, cols: Sequence[str], bins: int = 10) -> np.ndarray:\n",
    "    \"\"\"Weights ∝ 1/(tested_count_in_bin + 1) to favor under-sampled regions.\"\"\"\n",
    "    cand_key = _bin_keys(candidates, cols, bins=bins)\n",
    "    if tested is None or tested.empty:\n",
    "        weights = np.ones(len(candidates), dtype=float)\n",
    "        return (weights / weights.sum())\n",
    "    tested_key = _bin_keys(tested, cols, bins=bins)\n",
    "    counts = tested_key.value_counts()\n",
    "    weights = 1.0 / (counts.reindex(cand_key).fillna(0.0).to_numpy() + 1.0)\n",
    "    s = weights.sum()\n",
    "    return (weights / s) if s > 0 else np.full_like(weights, 1.0 / len(weights))\n",
    "\n",
    "\n",
    "# ---------- 3) Stratified (under-covered) + RSN ----------\n",
    "\n",
    "def sample_stratified_rsn(\n",
    "    conditions: Union[pd.DataFrame, np.ndarray],          # tested so far\n",
    "    reference_conditions: Union[pd.DataFrame, np.ndarray],# full grid / pool\n",
    "    num_samples: int = 1,\n",
    "    epsilon: float = 0.4,          # slightly higher explore rate\n",
    "    bins: int = 10,                # stratification granularity\n",
    "    subset_factor: int = 4,        # subset size ≈ subset_factor * num_samples\n",
    "    subset_cap: int = 200,\n",
    "    random_state: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Exploration: weighted random toward **under-covered bins** (1/(count+1)).\n",
    "    Exploitation: draw a **weighted subset** by those weights, then run RSN within it.\n",
    "    \"\"\"\n",
    "    # normalize inputs\n",
    "    reference_conditions = pd.DataFrame(reference_conditions).copy()\n",
    "    conditions = pd.DataFrame(conditions).copy()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # build candidate pool (exclude tested)\n",
    "    candidates = _anti_join(reference_conditions, conditions)\n",
    "    if candidates.empty:\n",
    "        return pd.DataFrame(columns=reference_conditions.columns).reset_index(drop=True)\n",
    "    if len(candidates) <= num_samples:\n",
    "        return candidates.reset_index(drop=True)\n",
    "\n",
    "    # scalers + weights\n",
    "    rc_min, rc_rng, num_cols = _grid_scalers(reference_conditions)\n",
    "    tested_arr = _scale(conditions, rc_min, rc_rng, num_cols)\n",
    "    weights = _under_coverage_weights(candidates, conditions, num_cols, bins=bins)\n",
    "    w_series = pd.Series(weights, index=candidates.index)\n",
    "\n",
    "    # cold start → weighted random\n",
    "    if conditions.empty:\n",
    "        return candidates.sample(n=num_samples, weights=w_series, random_state=random_state, replace=False).reset_index(drop=True)\n",
    "\n",
    "    # ε-explore (weighted)\n",
    "    if rng.random() < epsilon:\n",
    "        return candidates.sample(n=num_samples, weights=w_series, random_state=random_state, replace=False).reset_index(drop=True)\n",
    "\n",
    "    # exploit within a weighted subset, then greedy max–min\n",
    "    m = min(max(num_samples, subset_factor * num_samples), subset_cap, len(candidates))\n",
    "    sub = candidates.sample(n=m, weights=w_series, random_state=random_state, replace=False)\n",
    "\n",
    "    return _greedy_maxmin_subset(\n",
    "        subset_df=sub,\n",
    "        tested_arr=tested_arr,\n",
    "        rc_min=rc_min, rc_rng=rc_rng, cols=num_cols,\n",
    "        k=num_samples,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------- pick one strategy for your pipeline ----------\n",
    "# Default to RSN; switch to stratified by changing the alias.\n",
    "#sample = sample_rsn\n",
    "sample = sample_stratified_rsn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
